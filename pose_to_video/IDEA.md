# Sign Language Animation Reinvented: Controlling Existing Avatar Systems

## Introduction

Sign language animation serves as a vital tool for facilitating communication within the deaf and hard-of-hearing
community. Current state-of-the-art methods involve generating human skeletal poses and converting them into realistic
or 3D avatar animations. However, these techniques have their limitations. Realistic human animations may exhibit
temporal incoherencies and suffer from low visual quality, while requiring significant processing power and extensive
datasets. 3D avatars, on the other hand, are constrained to a limited set of pre-coded animation building blocks and
lack the realism of human figures.

We propose a novel framework for controlling animation in existing avatar systems by mapping pose estimations to vectors
that control the animations. These systems utilize quaternion rotation vectors or latent space vectors as control
inputs. Our approach involves using a pose estimation model to extract poses from the existing system's output and
training a mapping between the pose and the control vector.

## Methodology

### Realistic Human Generation

We suggest training an unconstrained StyleGAN3 model for generating realistic human figures. This system plays the role
of an existing realistic avatar system controlled by latent space. After training, we will create arbitrary videos using
latent space manipulation and perform pose estimation on the generated output. Subsequently, we will train a compact
model to convert the pose estimation vector into the latent space `W` vector.

Given existing poses, we can transform them into the latent `W` vector and generate new videos using StyleGAN3. We
hypothesize that the resulting output will exhibit all the benefits of the original animation system, although the
positioning may be less reliable.

For comparison purposes, we will employ a Pix2Pix baseline that generates reliable yet low-quality videos.

### 3D Avatars

For 3D avatars, we propose a similar methodology. We will animate the avatar using known animation sequences represented
as a tensor of quaternions, perform pose estimation, and learn a mapping from pose to quaternions. With a dataset of
poses, we will input them into the avatar, carry out pose estimation on the avatar, and train the model to map between
the predicted poses and the quaternions, iteratively refining the system.

We anticipate that observing the output will enable us to fine-tune the system with each iteration. Our primary
evaluation metric is the Mean Squared Error (MSE) loss, which will be assessed on a test set of human-made animations.

## Evaluation Metrics

To demonstrate the visual quality of our proposed approach, we will show figures with a video of the 3D avatar, and a
video of the realistic human. We do not report Fréchet Inception Distance, or Kernel Inception Distance since the output
quality is not what we are trying to measure.

Instead, we will mainly report the following metrics for both realistic human generation and 3D avatars:

- mse: Mean Squared Errors between input poses and extracted poses from the model output.
- inference_time: Time taken to perform inference on one pose video.

Given that we know existing avatar systems can generate high-quality videos, our primary research question is whether
our proposed approach will perform as well as the Pix2Pix model in terms of MSE - is it controllable.

To demonstrate the reliability of our proposed approach, we will show figures with a stream of poses, and a video of the
3D avatar, and a video of the realistic human with StyleGAN3, and with Pix2Pix.

## Evaluation Metrics

In order to showcase the visual quality of our proposed methodology, we will provide figures that include videos of both
the 3D avatar and the realistic human generated by our approach. Since our primary focus is not on measuring output
quality, we will not report the Fréchet Inception Distance or Kernel Inception Distance.

We will primarily report the following metric for both realistic human generation and 3D avatars:
Mean Squared Errors between input poses and extracted poses from the model output.
The original poses for this evaluation will be taken from the SignSuisse lexicon.

As existing avatar systems are already capable of producing high-quality videos, our main research question centers on
the controllability of our proposed method in comparison to the Pix2Pix model, as assessed by MSE performance.

To illustrate the dependability of our approach, we will present figures comprising a sequence of poses, along with
videos of the 3D avatar and realistic human generated using both StyleGAN3 and Pix2Pix.

## Conclusion

This research proposal aims to develop a novel framework for controlling existing animation systems by mapping pose
estimations to the corresponding control vectors, which are either quaternion rotation vectors or latent space vectors.
By refining this mapping process, we seek to overcome the limitations of current state-of-the-art methods in generating
sign language animations, which often result in low-quality outputs, temporal incoherencies, and limited realism. Our
proposed methodology will be rigorously evaluated through a range of metrics to assess its effectiveness in generating
high-quality sign language animations. Ultimately, we aspire to improve the communication experience for the deaf and
hard-of-hearing community by enhancing the quality and realism of sign language animations.
